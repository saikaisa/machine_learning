# 结果分析



## 线性回归函数

我们假设为 y = wx + b

定义 θ 为二维向量，θ = (b, w)



## LMS 闭式解

### 公式

$$
\theta = \left( X^T \cdot X \right)^{-1} \cdot X^T \cdot y
$$





## 梯度下降算法

### 公式

$$
损失函数（均方误差）：
L = \frac{1}{2n} \sum_{i=1}^{n} \left(y_i - (x_i \cdot \theta)\right)^2
$$

$$
梯度：
G = \frac{1}{n} \sum_{i=1}^{n} 2\left(x_i \cdot \theta - y_i\right) \cdot x_i
$$

$$
写成矩阵形式得：
G = \frac{2}{n} X^T \left(X \cdot \theta - y\right)
$$

$$
更新 θ 参数：
\theta' = \theta - \alpha \cdot G
$$

> α 为学习率



## 实验结果分析

> 梯度下降算法中，学习率为 0.01，迭代次数 1000 次。

<img src="https://pics.saikaisa.top/image-20231126181942040.png" alt="绘制结果" style="zoom: 67%;" />

![控制台输出](https://pics.saikaisa.top/image-20231126182024165-17009940324871-17009946778776.png)

由图可得，梯度下降算法中，学习率设置得恰当时（0.01），结果与闭式解算法几乎相同。

南京 2014 年的平均房价预估为 12.32 万元。

这比 2013 年的平均房价还要低，但通过坐标图易发现，线性拟合似乎并没有什么问题。只是在最后两年里，房价涨幅较大，这之后的数据可能不太适用线性回归。



接下来探究对于梯度下降算法，学习率不合适时的结果。

#### 学习率 = 0.001 时

<img src="https://pics.saikaisa.top/image-20231126183123839.png" alt="绘制结果" style="zoom:67%;" />

![控制台输出](https://pics.saikaisa.top/image-20231126183155732.png)

学习率过小，算法可能找到了另一个局部最优解，或者还没有找到局部最优解就停止了。



#### 学习率=0.1 时

![控制台输出](https://pics.saikaisa.top/image-20231126183254054.png)

学习率过大，怀疑其导致算法跳过最优解，无法得出结果。

