# 练习#1

## 基于BP算法的三层前向神经网络（不使用框架）

### 条件

#### 基本参数

- 学习率：0.01
- 迭代步数：5000
- K 倍交叉验证：K = 5
- 样本 X 经过了标准化

#### 选取的函数

- 输出层和隐藏层的激活函数：Sigmoid
- 代价函数（损失函数）：均方误差损失函数

#### 神经网络组成情况

- 输入层结点数：2
- 隐藏层结点数：5
- 输出层结点数：1



### 结果

![image-20231214202237040](https://pics.saikaisa.top/image-20231214202237040.png)

多次运行，准确率均在 72.5% 左右，效果还算可以。



### 与 Logistic 回归和 Softmax 回归的比较

**三层前向神经网络**：的复杂性更高，需要耗费更多的资源和时间，并且算法实现相对复杂，但是学习能力强，可以解决相对复杂的问题，处理复杂的模式和关系。

**Logistic回归**：比较简单，通常用于二元分类问题。计算需求较少，训练速度较快。使用 Sigmoid 激活函数，无法处理关系较为复杂的数据。

**Softmax回归**：是 Logistic 回归的推广，用于多类分类问题。计算需求较少，训练速度较快。





# 练习#2

## 基于BP算法的三层前向神经网络（使用 TensorFlow 框架）

### 可选条件

#### 基本参数

- 学习率：待定
- 迭代步数：待定
- K 倍交叉验证：K = 5
- 样本 X 经过了标准化

#### 选取的函数

- 输出层激活函数
  - Sigmoid：用于二元分类
  - Softmax：用于多类分类
- 隐藏层激活函数
  - Sigmoid
  - ReLU
  - Tanh
- 代价函数（损失函数）
  - mean_squared_error：均方误差函数
  - binary_crossentropy：二元交叉熵，用于二元分类
  - categorical_crossentropy：分类交叉熵，用于多类分类

#### 神经网络组成情况

- 输入层结点数：2
- 隐藏层层数：待定
- 隐藏层结点数：待定
- 输出层结点数：1



**通过修改以上参数获得了一些结果，接下来我将选取三种条件组合展示。**



### 结果

#### 第一种

##### 条件

<img src="https://pics.saikaisa.top/image-20231214204734035.png" alt="image-20231214204734035" style="zoom:50%;" />

##### 结果

![image-20231214203424663](https://pics.saikaisa.top/image-20231214203424663.png)

结果发现，在输出层激活函数使用 Softmax 的前提下，即使调整其它参数，准确率似乎也一直保持在 50% 不变。

考虑到这是个二元分类问题，所以 Softmax 可能不适合用于此问题。

因此，接下来将不再使用 Softmax 函数。



#### 第二种

##### 条件

<img src="前向神经网络结果分析.assets/image-20231214204359381.png" alt="image-20231214204359381" style="zoom: 50%;" />

##### 结果

![image-20231214204458851](https://pics.saikaisa.top/image-20231214204458851.png)

此条件下，迭代步数达到 5000 时，平均准确率能顺利提升到 70% 以上。



#### 第三种

##### 条件

<img src="https://pics.saikaisa.top/image-20231214205035841.png" alt="image-20231214205035841" style="zoom:50%;" />

##### 结果

![image-20231214205047992](https://pics.saikaisa.top/image-20231214205047992.png)

在该条件下，效果相对最好，达到了 75% 以上。